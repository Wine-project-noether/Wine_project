{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246f1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import prepare as prep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ae94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_wine():\n",
    "    \n",
    "    '''\n",
    "    This function gets the wine dataset from the csv file and drops unnecessary columns,\n",
    "    renames columns to make the data frame easier to read and removes outliers.\n",
    "    '''\n",
    "    \n",
    "    # creating initial Data Frame\n",
    "    df = pd.read_csv('wine_df.csv')\n",
    "    \n",
    "    # dropping columns\n",
    "    df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'sulphates'], inplace=True)\n",
    "    \n",
    "    # renaming columns\n",
    "    df.rename(columns={'fixed acidity':'fixed','volatile acidity':'volatile',\n",
    "                       'citric acid':'citric','residual sugar':'sugar','free sulfur dioxide':'fso2',\n",
    "                       'total sulfur dioxide':'tso2'}, inplace=True)\n",
    "    \n",
    "    # removing outliers\n",
    "    df, var_fences = prep.remove_outliers(df)\n",
    "    df = df[(df['quality']!=3) & (df['quality']!=9)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d8aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fso2_plot(df, col):\n",
    "    \n",
    "    '''\n",
    "    This function takes the data frame and the fso2 column and returns a catplot of amount of \n",
    "    free sulfur dioxide in each wine split up by quality.\n",
    "    '''\n",
    "    \n",
    "    sns.catplot(x='quality', y=col, data=df)\n",
    "    plt.axhline(df[col].mean(), linestyle='--', label='FSO2 Mean')\n",
    "    plt.ylabel('FSO2')\n",
    "    plt.title('FSO2 Content for Each Quality')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b4dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tso2_plot(df, col):\n",
    "    \n",
    "    '''\n",
    "    This function takes the data frame and the tso2 column and returns a bar plot of the \n",
    "    average total sulfur dioxide in the types of wines.\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    bplot = sns.barplot(x='type', y=col, data=df, palette='magma')\n",
    "    plt.axhline(df[col].mean(), linestyle='--', label='TSO2 Mean')\n",
    "    plt.xlabel('Wine Type')\n",
    "    plt.ylabel('TSO2')\n",
    "    ax.bar_label(bplot.containers[0], padding=7, fmt='%.2f')\n",
    "    plt.ylim(0,150)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa3bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def citric_plot(df, col):\n",
    "    \n",
    "    '''\n",
    "    This function takes the data frame and the citric column and returns a violin plot of the \n",
    "    amount of citric acid for each quality of wine.\n",
    "    '''\n",
    "    \n",
    "    sns.violinplot(x='quality', y=col, data=df, palette='Set2', saturation=1)\n",
    "    plt.axhline(df[col].mean(), linestyle='--', label='Citric Acid Mean')\n",
    "    plt.ylabel('Citric Content')\n",
    "    plt.xlabel('Quality')\n",
    "    plt.title('Comparing Citric Content to the Wine Quality')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3957e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def so2_ph_plot(df, col1, col2):\n",
    "    \n",
    "    '''\n",
    "    The function takes the data frame, the tso2 column and the pH column and returns a regression plot\n",
    "    of the correlation between total sulfur dioxide and pH level.\n",
    "    '''\n",
    "    \n",
    "    sns.regplot(x=col1, y=col2, data=df.sample(2000), line_kws={'color':'red'})\n",
    "    plt.title('Comparison of pH Level to Total Sulfur Dioxide')\n",
    "    plt.ylabel('Total Sulfur Dioxide')\n",
    "    plt.xlabel('pH Level')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85519dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(df, col, comp_col, comp_split):\n",
    "    \n",
    "    '''\n",
    "    This function takes a data frame, column, comparison column and where to split the comparison column.\n",
    "    It separates the comparison column on the split given and runs a one sample t-test comparing the \n",
    "    data of the column above the split to the overall mean.\n",
    "    '''\n",
    "    \n",
    "    alpha = .05\n",
    "    \n",
    "    # determining the sample and the overall mean\n",
    "    sample = df[df[comp_col]>=comp_split][col]\n",
    "    overall_mean = df[col].mean()\n",
    "    \n",
    "    # running the ttest \n",
    "    t, p = stats.ttest_1samp(sample, overall_mean)\n",
    "    \n",
    "    # if statement to determine whether t value needs to be above or below 0\n",
    "    if (p/2 < alpha) and (t > 0):\n",
    "        print(\"We reject the null.\")\n",
    "    else:\n",
    "        print(\"We fail to reject the null.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e843f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_type(df, col, comp_col, comp_split):    \n",
    "    \n",
    "    '''\n",
    "    This function takes a data frame, column, comparison column and where to split the comparison column.\n",
    "    It separates the comparison column on the split given and runs a one sample t-test comparing the \n",
    "    data of the column above the split to the overall mean.\n",
    "    '''\n",
    "    \n",
    "    alpha = .05\n",
    "    \n",
    "    # determining the sample and the overall mean\n",
    "    sample = df[df[comp_col]==comp_split][col]\n",
    "    overall_mean = df[col].mean()\n",
    "    \n",
    "    # running the ttest \n",
    "    t, p = stats.ttest_1samp(sample, overall_mean)\n",
    "    \n",
    "    # if statement to determine whether t value needs to be above or below 0\n",
    "    if (p/2 < alpha) and (t < 0):\n",
    "        print(\"We reject the null.\")\n",
    "    else:\n",
    "        print(\"We fail to reject the null.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1866bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_test(df, col1, col2):\n",
    "    \n",
    "    '''\n",
    "    This function takes a data frame and 2 columns and runs a pearsonr stats test on the columns and \n",
    "    returns the correlation between the two columns.\n",
    "    '''\n",
    "    \n",
    "    alpha = .05\n",
    "    \n",
    "    # running stats test on both columns\n",
    "    corr, p = stats.pearsonr(df[col1], df[col2])\n",
    "    \n",
    "    # if statment to determine whether to reject the null\n",
    "    if p < alpha:\n",
    "        print(\"We reject the null.\")\n",
    "        print(f'{col1} and {col2} have a correlation of {corr}')\n",
    "    else:\n",
    "        print(\"We fail to reject the null.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06321b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scale(df):\n",
    "    \n",
    "    '''\n",
    "    This function gets dummies for the type column, splits and scales the data.\n",
    "    '''\n",
    "    \n",
    "    # creating list of numeric columns and getting dummies\n",
    "    col_list = df.select_dtypes(include=['float64','int64']).columns[:-1]\n",
    "    df = pd.get_dummies(df, columns=['type'])\n",
    "    \n",
    "    # splitting and scaling data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = prep.x_y_split(df, 'quality')\n",
    "    X_train, X_val, X_test = prep.mm_scaler(X_train, X_val, X_test, col_list)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68744fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(X_train, cluster_vars, k_range):\n",
    "    \n",
    "    '''\n",
    "    This function takes the X_trian data set and the variables to cluster on and creates a chart\n",
    "    of the interia of the k value to determine which k value is the best.\n",
    "    '''\n",
    "    \n",
    "    sse = []\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "        # X[0] is our X_train dataframe..the first dataframe in the list of dataframes stored in X. \n",
    "        kmeans.fit(X_train[cluster_vars])\n",
    "\n",
    "        # inertia: Sum of squared distances of samples to their closest cluster center.\n",
    "        sse.append(kmeans.inertia_) \n",
    "\n",
    "    # compute the difference from one k to the next\n",
    "    delta = [round(sse[i] - sse[i+1],0) for i in range(len(sse)-1)]\n",
    "\n",
    "    # compute the percent difference from one k to the next\n",
    "    pct_delta = [round(((sse[i] - sse[i+1])/sse[i])*100, 1) for i in range(len(sse)-1)]\n",
    "\n",
    "    # create a dataframe with all of our metrics to compare them across values of k: SSE, delta, pct_delta\n",
    "    k_comparisons_df = pd.DataFrame(dict(k=k_range[0:-1], \n",
    "                             sse=sse[0:-1], \n",
    "                             delta=delta, \n",
    "                             pct_delta=pct_delta))\n",
    "\n",
    "    # plot k with pct_delta\n",
    "    plt.plot(k_comparisons_df.k, k_comparisons_df.pct_delta, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Percent Change')\n",
    "    plt.title('For which k values are we seeing increased changes (%) in SSE?')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a424774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(X, k, cluster_vars):\n",
    "    \n",
    "    '''\n",
    "    This function creates the clusters using KMeans.\n",
    "    '''\n",
    "    \n",
    "    # create kmean object\n",
    "    kmeans = KMeans(n_clusters=k, random_state = 42)\n",
    "\n",
    "    # fit to train and assign cluster ids to observations\n",
    "    kmeans.fit(X[0][cluster_vars])\n",
    "\n",
    "    return kmeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4de00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(kmeans, cluster_vars, cluster_name):\n",
    "    \n",
    "    '''\n",
    "    This function creates a dataframe of the centroids for the clusters\n",
    "    '''\n",
    "    \n",
    "    # get the centroids for each distinct cluster...\n",
    "\n",
    "    centroid_col_names = ['centroid_' + i for i in cluster_vars]\n",
    "\n",
    "    centroid_df = pd.DataFrame(kmeans.cluster_centers_, \n",
    "                               columns=centroid_col_names).reset_index().rename(columns={'index': cluster_name})\n",
    "\n",
    "    return centroid_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b68159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(kmeans, cluster_vars, cluster_name, centroid_df, X):\n",
    "    \n",
    "    '''\n",
    "    This function adds the clusters and centroids to the X dataframes.\n",
    "    '''\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        clusters = pd.DataFrame(kmeans.predict(X[i][cluster_vars]), \n",
    "                            columns=[cluster_name], index=X[i].index)\n",
    "\n",
    "        clusters_centroids = clusters.merge(centroid_df, on=cluster_name, copy=False).set_index(clusters.index.values)\n",
    "\n",
    "        X[i] = pd.concat([X[i], clusters_centroids], axis=1)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbee7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_so2(X):\n",
    "    \n",
    "    '''\n",
    "    This function creates the clusters for the sulfur dioxide variables.\n",
    "    '''\n",
    "    \n",
    "    # creating cluster variables and name\n",
    "    cluster_vars = ['fso2', 'tso2']\n",
    "    cluster_name = 'so2_cluster'\n",
    "    \n",
    "    k = 5\n",
    "    \n",
    "    # creatings clusters\n",
    "    kmeans = create_clusters(X, k, cluster_vars)\n",
    "    \n",
    "    # creating centroids\n",
    "    centroid_df_so2 = get_centroids(kmeans, cluster_vars, cluster_name)\n",
    "    \n",
    "    # adding clusters and centroids to X data frames\n",
    "    X = assign_clusters(kmeans, cluster_vars, cluster_name, centroid_df_so2, X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc181be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_acids(X):\n",
    "    \n",
    "    '''\n",
    "    This function creates the clusters for the acid variables.\n",
    "    '''\n",
    "    \n",
    "    # creating cluster variables and name\n",
    "    cluster_vars = ['fixed', 'volatile', 'citric']\n",
    "    cluster_name = 'acids_cluster'\n",
    "    \n",
    "    k = 3\n",
    "    \n",
    "    # creating clusters\n",
    "    kmeans = create_clusters(X, k, cluster_vars)\n",
    "    \n",
    "    #creating centroids\n",
    "    centroid_df_acid = get_centroids(kmeans, cluster_vars, cluster_name)\n",
    "    \n",
    "    # adding clusters and centroids to X data frames\n",
    "    X = assign_clusters(kmeans, cluster_vars, cluster_name, centroid_df_acid, X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "605cbbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_visc(X):\n",
    "    \n",
    "    '''\n",
    "    This function creates the clusters for the viscosity variables.\n",
    "    '''\n",
    "    \n",
    "    # creating cluster variables and name\n",
    "    cluster_vars = ['density', 'alcohol']\n",
    "    cluster_name = 'visc_cluster'\n",
    "    \n",
    "    k = 5 \n",
    "    \n",
    "    # creating clusters\n",
    "    kmeans = create_clusters(X, k, cluster_vars)\n",
    "    \n",
    "    # creating centroids\n",
    "    centroid_df_visc = get_centroids(kmeans, cluster_vars, cluster_name)\n",
    "    \n",
    "    # addign clusters and centroids to X data frames\n",
    "    X = assign_clusters(kmeans, cluster_vars, cluster_name, centroid_df_visc, X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e02e8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tso2_cluster_plot(X, y_train, col, cluster):\n",
    "    \n",
    "    '''\n",
    "    This function plots a stripplot for total sulfur dioxide compared to quality, hued with the cluster\n",
    "    '''\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.stripplot(x=y_train, y=X[0][col], hue=X[0][cluster])\n",
    "    plt.ylabel('Total Sulfur Dioxide')\n",
    "    plt.xlabel('Quality')\n",
    "    plt.title('Is there a distinction between clusters when visualizing TSO2 and Quality?')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c712162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acid_cluster_plot(X, y_train, col, cluster):\n",
    "\n",
    "    '''\n",
    "    This function plots a stripplot for acidity compared to quality, hued with the cluster\n",
    "    '''\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.stripplot(x=y_train, y=X[0][col], hue=X[0][cluster])\n",
    "    plt.ylabel('Acidity')\n",
    "    plt.xlabel('Quality')\n",
    "    plt.title('Is there a distinction between clusters when visualizing Acidity and Quality?')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a664ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visc_cluster_plot(X, y_train, col, cluster):\n",
    "    \n",
    "    '''\n",
    "    This function plots a stripplot for density compared to quality, hued with the cluster\n",
    "    '''\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.stripplot(x=y_train, y=X[0][col], hue=X[0][cluster])\n",
    "    plt.title('Is there a distinction between clusters when visualizing Density and Quality?')\n",
    "    plt.ylabel('Alcohol Amount')\n",
    "    plt.xlabel('Quality')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50001df0",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe657fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    Takes in x and y to make a plot for each score of x and y train\n",
    "    '''\n",
    "    \n",
    "    tree_train = []\n",
    "    tree_val = []\n",
    "    depth = []\n",
    "    for i in range(2, 21):\n",
    "        train_tree = DecisionTreeClassifier(max_depth=i, random_state=42)\n",
    "        train_tree = train_tree.fit(X_train, y_train)\n",
    "\n",
    "        tree_train.append(train_tree.score(X_train, y_train))\n",
    "        tree_val.append(train_tree.score(X_val, y_val))\n",
    "        depth.append(i)\n",
    "    \n",
    "        # Creating the rfc_dataframe\n",
    "    tree_scores = pd.DataFrame({'score':tree_train,\n",
    "                           'type':'train',\n",
    "                           'depth':depth})\n",
    "    #Creating val_rfc_score_df\n",
    "    val_tree_scores = pd.DataFrame({'score':tree_val,\n",
    "                                   'type':'val',\n",
    "                                   'depth':depth})\n",
    "    #Creating rfc_score_df\n",
    "    tree_scores = tree_scores.append(val_tree_scores)\n",
    "    # train scores loc of ref_score_df\n",
    "    train_acc=tree_scores.loc[tree_scores['type'] == 'train']\n",
    "    # val scores loc of ref_score_df\n",
    "    val_acc=tree_scores.loc[tree_scores['type'] == 'val']\n",
    "    # train depth loc of rfc_score_df\n",
    "    train_depth=tree_scores.loc[tree_scores['type']== 'train']['depth']\n",
    "    # val depth loc of rfc_score_df\n",
    "    val_depth=tree_scores.loc[tree_scores['type']== 'val']['depth']\n",
    "    # rfc_scre_df \n",
    "    tree_score_df= pd.DataFrame(\n",
    "        {'train_score': train_acc.score,\n",
    "        'val_score': val_acc.score,\n",
    "         'train_depth': train_depth,\n",
    "         'val_depth' : val_depth\n",
    "        })\n",
    "    # plot f, ax\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    \n",
    "    #setting the title of chart\n",
    "    plt.title(\"Decision Tree Accuracy and Depth Chart\")\n",
    "    # plotting the data\n",
    "    sns.pointplot(data =tree_score_df, x='train_depth', y='train_score', label='Train',color='royalblue')\n",
    "    sns.pointplot(data =tree_score_df, x='val_depth', y='val_score', label='Val', color=\"seagreen\")\n",
    "    # setting the labels\n",
    "    plt.ylabel('score')\n",
    "    plt.xlabel('depth')\n",
    "    #Showing the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8e44a",
   "metadata": {},
   "source": [
    "# Random Forest Classifier model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57540f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_for_class(X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    Takes in x and y train to make a plot of each score of xand y train\n",
    "    '''\n",
    "    rfc_train = [] #Empty List\n",
    "    rfc_val = []\n",
    "    depth = []\n",
    "    \n",
    "    for i in range(2, 21): # Range of 2-21 and each value inbetween\n",
    "        # Random Forest Classifier\n",
    "        rf = RandomForestClassifier(bootstrap=True, \n",
    "                                    class_weight=None, \n",
    "                                    criterion='gini',\n",
    "                                    min_samples_leaf=3,\n",
    "                                    n_estimators=100,\n",
    "                                    max_depth=i, \n",
    "                                    random_state=42)\n",
    "        # Fitting the Xtrain and ytrain for the model\n",
    "        rf.fit(X_train, y_train)\n",
    "        # Creating values for empty lists\n",
    "        rfc_train.append(rf.score(X_train, y_train))\n",
    "        rfc_val.append(rf.score(X_val, y_val))\n",
    "        depth.append(i)\n",
    "    # Creating the rfc_dataframe\n",
    "    rfc_scores = pd.DataFrame({'score':rfc_train,\n",
    "                           'type':'train',\n",
    "                           'depth':depth})\n",
    "    #Creating val_rfc_score_df\n",
    "    val_rfc_scores = pd.DataFrame({'score':rfc_val,\n",
    "                                   'type':'val',\n",
    "                                   'depth':depth})\n",
    "    #Creating rfc_score_df\n",
    "    rfc_scores = rfc_scores.append(val_rfc_scores)\n",
    "    # train scores loc of ref_score_df\n",
    "    train_acc=rfc_scores.loc[rfc_scores['type'] == 'train']\n",
    "    # val scores loc of ref_score_df\n",
    "    val_acc=rfc_scores.loc[rfc_scores['type'] == 'val']\n",
    "    # train depth loc of rfc_score_df\n",
    "    train_depth=rfc_scores.loc[rfc_scores['type']== 'train']['depth']\n",
    "    # val depth loc of rfc_score_df\n",
    "    val_depth=rfc_scores.loc[rfc_scores['type']== 'val']['depth']\n",
    "    # rfc_scre_df \n",
    "    rfc_score_df= pd.DataFrame(\n",
    "        {'train_score': train_acc.score,\n",
    "        'val_score': val_acc.score,\n",
    "         'train_depth': train_depth,\n",
    "         'val_depth' : val_depth\n",
    "        })\n",
    "    # plot f, ax\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    \n",
    "    #setting the title of chart\n",
    "    plt.title(\"Random Forest Classifier Accuracy and Depth Chart\")\n",
    "    # plotting the data\n",
    "    sns.pointplot(data =rfc_score_df, x='train_depth', y='train_score', label='Train',color='royalblue')\n",
    "    sns.pointplot(data =rfc_score_df, x='val_depth', y='val_score', label='Val', color=\"seagreen\")\n",
    "    # setting the labels\n",
    "    plt.ylabel('score')\n",
    "    plt.xlabel('depth')\n",
    "    #Showing the graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26002b7",
   "metadata": {},
   "source": [
    "# XGB Classifer Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f928d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_score(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    depth = []\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_val = le.transform(y_val)\n",
    "    y_test = le.transform(y_test)\n",
    "    \n",
    "    for i in range(2,10):\n",
    "        \n",
    "        xgb = XGBClassifier(objective='multi:softmax',\n",
    "                           seed=42,\n",
    "                           max_depth=i,\n",
    "                           learning_rate=.2,\n",
    "                           gamma=.5,\n",
    "                           reg_alpha=.75,\n",
    "                           reg_lambda=.25,\n",
    "                           min_child_weight=5,\n",
    "                           max_leaves=4,\n",
    "                           subsample=.6,\n",
    "                           n_estimators=300)\n",
    "\n",
    "        xgb.fit(X_train, y_train)\n",
    "        \n",
    "        train_scores.append(xgb.score(X_train, y_train))\n",
    "        val_scores.append(xgb.score(X_val, y_val))\n",
    "        depth.append(i)\n",
    "        \n",
    "        \n",
    "    xgb_scores = pd.DataFrame({'score':train_scores,\n",
    "                               'type':'train',\n",
    "                               'depth':depth})\n",
    "    #Creating val_rfc_score_df\n",
    "    val_xgb_scores = pd.DataFrame({'score':val_scores,\n",
    "                                   'type':'val',\n",
    "                                   'depth':depth})\n",
    "    \n",
    "    xgb_scores = xgb_scores.append(val_xgb_scores)\n",
    "    \n",
    "    train_acc=xgb_scores.loc[xgb_scores['type'] == 'train']\n",
    "    # val scores loc of ref_score_df\n",
    "    xgb_acc=xgb_scores.loc[xgb_scores['type'] == 'val']\n",
    "    # train depth loc of rfc_score_df\n",
    "    train_depth=xgb_scores.loc[xgb_scores['type']== 'train']['depth']\n",
    "    # val depth loc of rfc_score_df\n",
    "    val_depth=xgb_scores.loc[xgb_scores['type']== 'val']['depth']\n",
    "    # rfc_scre_df \n",
    "    xgb_score_df= pd.DataFrame(\n",
    "        {'train_score': train_acc.score,\n",
    "        'val_score': val_acc.score,\n",
    "         'train_depth': train_depth,\n",
    "         'val_depth' : val_depth\n",
    "        })\n",
    "\n",
    "    plt.title('XGBoost Classifier Accuracy and Depth Chart')\n",
    "    \n",
    "    sns.pointplot(data =xgb_score_df, x='train_depth', y='train_score', label='Train',color='royalblue')\n",
    "    sns.pointplot(data =xgb_score_df, x='val_depth', y='val_score', label='Val', color=\"seagreen\")\n",
    "    \n",
    "    # setting the labels\n",
    "    plt.ylabel('score')\n",
    "    plt.xlabel('depth')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d357a54",
   "metadata": {},
   "source": [
    "# KNN Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beb998e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_scores(X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    Takes in x-train, y-train, x-val, and y-val \n",
    "    to make a plot of each score of x/y train and x/y val\n",
    "    '''\n",
    "    #Empty lists\n",
    "    knn_train = []\n",
    "    knn_val = []\n",
    "    depth = []\n",
    "    # Range of 2-21 and each value inbetween\n",
    "    for i in range(2, 51):\n",
    "         # KNN Classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=i, weights='uniform')\n",
    "        #fitting the model\n",
    "        knn.fit(X_train, y_train)\n",
    "        #y_pred array from prediciting X_train\n",
    "        y_pred = knn.predict(X_train)\n",
    "        #probability of y_pred array from prediciting X_train\n",
    "        y_pred_proba = knn.predict_proba(X_train)\n",
    "        #adding values to depth list\n",
    "        depth.append(i)\n",
    "        # adding values to knn_train and knn.val lists\n",
    "        knn_train.append(knn.score(X_train, y_train))\n",
    "        knn_val.append(knn.score(X_val, y_val))\n",
    "    # knn_scores DataFrame\n",
    "    knn_scores = pd.DataFrame({'score':knn_train,\n",
    "                                   'type':'train',\n",
    "                                   'depth':depth})\n",
    "    #val_knn_scores DataFrame\n",
    "    val_knn_scores = pd.DataFrame({'score':knn_val,\n",
    "                                       'type':'val',\n",
    "                                       'depth':depth})\n",
    "    # Knn scores values added to val_knn_scores DataFrame\n",
    "    knn_scores = knn_scores.append(val_knn_scores)\n",
    "    # Creating the acc_dataframe from values of above DataFrames\n",
    "    acc_df= pd.DataFrame(\n",
    "            {'train_depth': knn_scores.depth[0:49], \n",
    "             'val_depth' : val_knn_scores.depth[0:49],\n",
    "             'train_knn_score': knn_scores.score.values[0:49],\n",
    "            'val_knn_score': val_knn_scores.score.values[0:49]\n",
    "                  })\n",
    "    # plot f, ax\n",
    "    f, ax=plt.subplots(1,1)\n",
    "    #setting the title of chart\n",
    "    plt.title(\"KNN Scores Accuracy with Depth\")\n",
    "    # plotting the data\n",
    "    sns.pointplot(x='train_depth', y='train_knn_score', data=acc_df, color='royalblue', label='Train')\n",
    "    sns.pointplot(x='val_depth', y='val_knn_score', data=acc_df, color='seagreen', label='Val')\n",
    "    # setting the labels\n",
    "    plt.xlabel('Depth')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlim(0, 20)\n",
    "    #Showing the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ff62e",
   "metadata": {},
   "source": [
    "# Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae4593dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_function_train_val(X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    Function that returns the plot the accuracy of each model with the x/y train and x/y validate\n",
    "    '''\n",
    "    tree = decision_tree(X_train, y_train, X_val, y_val)\n",
    "    rfc = random_for_class(X_train, y_train, X_val, y_val)\n",
    "    xgb = xgb_score(X_train, y_train, X_val, y_val)\n",
    "    knn = knn_scores(X_train, y_train, X_val, y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d04b4",
   "metadata": {},
   "source": [
    "# Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca3014ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    Test Score function that \n",
    "    shows the Test Score Accuracy from XGB Classifier\n",
    "    '''\n",
    "    # XGB Boost Classifer\n",
    "    xgb = XGBClassifier(objective='multi:softmax',\n",
    "                           seed=42,\n",
    "                           max_depth=8,\n",
    "                           learning_rate=.2,\n",
    "                           gamma=.5,\n",
    "                           reg_alpha=.75,\n",
    "                           reg_lambda=.25,\n",
    "                           min_child_weight=5,\n",
    "                           max_leaves=4,\n",
    "                           subsample=.6,\n",
    "                           n_estimators=300)\n",
    "    \n",
    "    # fitting the model on the x/y-train\n",
    "    xgb.fit(X_train, y_train)\n",
    "    # Score of the x/y-test\n",
    "    score= xgb.score(X_test, y_test)\n",
    "    #returning the score\n",
    "    print(f'The final test score is {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d5fa0",
   "metadata": {},
   "source": [
    "# Test Against Baseline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dc45306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    Test Score Against Baseline function that \n",
    "    shows the plot of the Test Score Accuracy from RFC to our Baseline Model\n",
    "    '''\n",
    "    # XGB Boost Classifer\n",
    "    xgb = XGBClassifier(objective='multi:softmax',\n",
    "                           seed=42,\n",
    "                           max_depth=8,\n",
    "                           learning_rate=.2,\n",
    "                           gamma=.5,\n",
    "                           reg_alpha=.75,\n",
    "                           reg_lambda=.25,\n",
    "                           min_child_weight=5,\n",
    "                           max_leaves=4,\n",
    "                           subsample=.6,\n",
    "                           n_estimators=300)\n",
    "    \n",
    "    # fitting the model on the x/y-train\n",
    "    xgb.fit(X_train, y_train)\n",
    "    # Score of the x/y-test\n",
    "    score= xgb.score(X_test, y_test)\n",
    "    \n",
    "    baseline = (wine['quality']==6).mean()\n",
    "    # Creating DataFrame of test score and baseline columns and values\n",
    "    test_base_df=pd.DataFrame({'Test Score': [score],\n",
    "                          'Baseline': [baseline]})\n",
    "    # plot f, ax\n",
    "    f, ax = plt.subplot()\n",
    "    #Setting the title of chart\n",
    "    plt.title(\"Test Score Against Baseline Score\")\n",
    "    # plotting the data\n",
    "    sns.barplot(data=test_base_df, palette='viridis')\n",
    "    # setting the labels\n",
    "    plt.ylabel('Score')\n",
    "    #Showing the graph\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
